# üìÑ –≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ö–æ–¥–∏—Ç —Å–∞–π—Ç, –Ω–∞—á–∏–Ω–∞—è —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã:
# - –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π
# - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞ –≤ —Ñ–∞–π–ª `structure.txt`
# - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤ —Ñ–∞–π–ª `output.txt`
# - –†–∞–±–æ—Ç–∞–µ—Ç —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏ –∏–∑–±–µ–≥–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ—Å–µ—â–µ–Ω–∏—è –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin


def extract_text_and_structure(url, depth=0, visited=None, structure_file="structure.txt", output_file="output.txt"):
    if visited is None:
        visited = set()

    if url in visited:
        return
    visited.add(url)

    response = requests.get(url)
    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')

    if depth == 0:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ h1 –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        h1_tag = soup.find('h1')
        category_name = h1_tag.text.strip() if h1_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ structure.txt
        with open(structure_file, "a", encoding="utf-8") as struct_file:
            struct_file.write(f"- {category_name}\n")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ div —Å –Ω—É–∂–Ω—ã–º –∫–ª–∞—Å—Å–æ–º
        description = []
        info_div = soup.find('div', class_='category info text-left')
        if info_div:
            paragraphs = info_div.find_all('p')
            description = [p.text.strip() for p in paragraphs if p.text.strip()]

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ output.txt
        with open(output_file, "a", encoding="utf-8") as out_file:
            out_file.write(f"{'=' * 50}\n")
            out_file.write(f"{category_name}\n\n")
            out_file.write("\n".join(description) + "\n" if description else "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç\n")
            out_file.write(f"{'=' * 50}\n\n")

        # –ò—â–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        category_items = soup.select('.categories-list .card')
        for item in category_items:
            category_url = urljoin(url, item.get('href'))
            category_name = item.select_one('.card-title').text.strip()

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ structure.txt
            with open(structure_file, "a", encoding="utf-8") as struct_file:
                struct_file.write(f"  - {category_name}\n")

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            time.sleep(1)  # –î–∞–µ–º –ø–∞—É–∑—É, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–∞–π—Ç
            extract_text_and_structure(category_url, depth + 1, visited, structure_file, output_file)
    else:
        # –î–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
        category_name = soup.find('title').text.strip()

    # –ò—â–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Å—Å—ã–ª–∫–∏ —Å –Ω—É–∂–Ω—ã–º –∫–ª–∞—Å—Å–æ–º)
    subcategories = soup.find_all('a', class_='btn btn-sm btn-outline-secondary')
    for sub in subcategories:
        sub_url = sub.get('href')
        sub_name = sub.text.strip()

        if sub_url:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É –≤ –∞–±—Å–æ–ª—é—Ç–Ω—É—é
            full_sub_url = urljoin(url, sub_url)

            if full_sub_url not in visited:
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ structure.txt
                indent = "  " * (depth + 1)
                with open(structure_file, "a", encoding="utf-8") as struct_file:
                    struct_file.write(f"{indent}- {sub_name}\n")

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ div —Å –Ω—É–∂–Ω—ã–º –∫–ª–∞—Å—Å–æ–º
                description = []
                info_div = soup.find('div', class_='category info text-left')
                if info_div:
                    paragraphs = info_div.find_all('p')
                    description = [p.text.strip() for p in paragraphs if p.text.strip()]

                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏) –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ output.txt
                with open(output_file, "a", encoding="utf-8") as out_file:
                    out_file.write(f"{'=' * 50}\n")
                    out_file.write(f"{sub_name}\n\n")
                    out_file.write("\n".join(description) + "\n" if description else "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç\n")
                    out_file.write(f"{'=' * 50}\n\n")

                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é
                time.sleep(1)  # –î–∞–µ–º –ø–∞—É–∑—É, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–∞–π—Ç
                extract_text_and_structure(full_sub_url, depth + 1, visited, structure_file, output_file)


if __name__ == "__main__":
    start_url = input("–í–≤–µ–¥–∏—Ç–µ URL —Å–∞–π—Ç–∞: ").strip()

    # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª—ã –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã
    open("structure.txt", "w", encoding="utf-8").close()
    open("output.txt", "w", encoding="utf-8").close()

    extract_text_and_structure(start_url)
    print("–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω!")
