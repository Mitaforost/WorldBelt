# —Ä–∞–Ω–¥–æ–º–Ω–æ –æ—Ç–±–∏—Ä–∞–µ—Ç 3 —Ç–æ–≤–∞—Ä–∞ –±–µ–∑ –≤–æ–ø—Ä–æ—Å–æ–≤

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import random

def collect_random_no_question_links(start_url):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)

    try:
        print(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {start_url}")
        driver.get(start_url)
        time.sleep(2)

        # –ù–∞–∂–∏–º–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å –µ—â—ë", –ø–æ–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞
        while True:
            try:
                show_more = driver.find_element(By.XPATH, '//button[@radicalmart-ajax-paginaton="button"]')
                driver.execute_script("arguments[0].click();", show_more)
                time.sleep(1)
            except:
                break

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        product_links = [urljoin(start_url, a['href']) for a in soup.select('a.h6.text-decoration-none') if a.get('href')]
        print(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(product_links)} —Ç–æ–≤–∞—Ä–æ–≤.")

        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫
        random.shuffle(product_links)

        found = 0
        max_needed = 3

        with open("questions.txt", "w", encoding="utf-8") as questions_file:
            for i, full_url in enumerate(product_links, 1):
                if found >= max_needed:
                    break

                print(f"\nüõí –ü—Ä–æ–≤–µ—Ä–∫–∞: {full_url}")

                try:
                    driver.get(full_url)

                    # –ñ–¥—ë–º, –Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ ‚Äî –Ω–µ –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –µ–≥–æ –∏–º–µ—é—Ç
                    try:
                        WebDriverWait(driver, 2).until(
                            EC.presence_of_element_located((By.ID, "comments-form"))
                        )
                    except:
                        pass  # —Ñ–æ—Ä–º–∞ –º–æ–∂–µ—Ç –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è ‚Äî —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ

                    product_soup = BeautifulSoup(driver.page_source, 'html.parser')
                    has_form = product_soup.find('form', id='comments-form')
                    comments_div = product_soup.find('div', id='comments')
                    has_comments = comments_div and comments_div.find()

                    if not (has_form and has_comments):
                        questions_file.write(full_url + "\n")
                        print("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ (–≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ—Ç)")
                        found += 1
                    else:
                        print("‚è≠Ô∏è –í–æ–ø—Ä–æ—Å—ã –µ—Å—Ç—å ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")

                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                    continue

        print(f"\nüìÅ –ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–µ–Ω–æ {found} —Ç–æ–≤–∞—Ä–æ–≤ –±–µ–∑ –≤–æ–ø—Ä–æ—Å–æ–≤. –°—Å—ã–ª–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ questions.txt")

    finally:
        driver.quit()


# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    url = input("–í–≤–µ–¥–∏—Ç–µ URL –∫–∞—Ç–∞–ª–æ–≥–∞ —Å —Ç–æ–≤–∞—Ä–∞–º–∏: ").strip()
    collect_random_no_question_links(url)
