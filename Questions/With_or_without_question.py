from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

def check_all_products_with_comments(start_url):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)

    with_questions = 0
    without_questions = 0

    with open("questions.txt", "w", encoding="utf-8") as questions_file:

        try:
            print(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {start_url}")
            driver.get(start_url)
            time.sleep(2)

            # –ù–∞–∂–∏–º–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å –µ—â—ë", –ø–æ–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞
            while True:
                try:
                    show_more = driver.find_element(By.XPATH, '//button[@radicalmart-ajax-paginaton="button"]')
                    driver.execute_script("arguments[0].click();", show_more)
                    time.sleep(1)
                except:
                    break

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            product_links = [a['href'] for a in soup.select('a.h6.te    xt-decoration-none') if a.get('href')]
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(product_links)} —Ç–æ–≤–∞—Ä–æ–≤. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –±–ª–æ–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤...")

            for i, link in enumerate(product_links, 1):
                full_url = urljoin(start_url, link)
                print(f"\nüõí [{i}/{len(product_links)}] –ü—Ä–æ–≤–µ—Ä–∫–∞: {full_url}")

                try:
                    driver.get(full_url)

                    # –ñ–¥—ë–º —Ñ–æ—Ä–º—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –ø–æ—è–≤–ª—è–µ—Ç—Å—è
                    WebDriverWait(driver, 3).until(
                        EC.presence_of_element_located((By.ID, "comments-form"))
                    )

                    product_soup = BeautifulSoup(driver.page_source, 'html.parser')
                    has_form = product_soup.find('form', id='comments-form')
                    comments_div = product_soup.find('div', id='comments')
                    has_comments = comments_div and comments_div.find()

                    if has_form and has_comments:
                        with_questions += 1
                        
                        print("‚úÖ –í–æ–ø—Ä–æ—Å—ã –µ—Å—Ç—å")
                    else:
                        without_questions += 1
                        questions_file.write(full_url + "\n")
                        print("‚ùå –í–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ—Ç")

                except Exception as e:
                    without_questions += 1
                    print(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–æ—Ä–º—ã –∏–ª–∏ –≤–æ–ø—Ä–æ—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–∏–ª–∏ –æ—à–∏–±–∫–∞): {e}")

        finally:
            driver.quit()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏:")
    print(f"‚Äî –° –≤–æ–ø—Ä–æ—Å–∞–º–∏: {with_questions}")
    print(f"‚Äî –ë–µ–∑ –≤–æ–ø—Ä–æ—Å–æ–≤: {without_questions}")
    print(f"‚Äî –í—Å–µ–≥–æ: {with_questions + without_questions}")
    print("üìÅ –í—Å–µ —Ç–æ–≤–∞—Ä—ã –±–µ–∑ –≤–æ–ø—Ä–æ—Å–æ–≤ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ questions.txt")

# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    url = input("–í–≤–µ–¥–∏—Ç–µ URL –∫–∞—Ç–∞–ª–æ–≥–∞ —Å —Ç–æ–≤–∞—Ä–∞–º–∏: ").strip()
    check_all_products_with_comments(url)
